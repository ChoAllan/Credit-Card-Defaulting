import os

get_ipython().run_line_magic("matplotlib", " inline")
import sys

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import xgboost as xgb
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.dummy import DummyClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    f1_score,
    make_scorer,
    plot_confusion_matrix,
)
from sklearn.model_selection import (
    GridSearchCV,
    RandomizedSearchCV,
    cross_val_score,
    cross_validate,
    train_test_split,
)
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler
from sklearn.svm import SVC


# reading dataset from CSV file
cc_df = pd.read_csv("data/UCI_Credit_Card.csv")


# splitting of data into train and test portions

X = cc_df.drop(columns = ["default.payment.next.month", "ID"])
y = cc_df["default.payment.next.month"]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=123
)


cc_df.describe()


pay = cc_df[cc_df['default.payment.next.month'] >= 1]

nopay = cc_df[cc_df['default.payment.next.month'] == 0]

print("pay: " + str(pay.shape))
print("nopay: " + str(nopay.shape))


features = []

for col in cc_df.columns:
    features.append(col)

figures = dict()

for feature in features:
    plt.hist(nopay[feature], alpha=1, bins=50, label="0")
    plt.hist(pay[feature], alpha=0.5, bins=50, label="1")
    plt.legend(loc="best")

    plt.xlabel(feature)
    plt.ylabel("count")
    plt.title(f"Histogram of {feature} by target class")
    plt.show()


cc_df.plot(
    kind='scatter',
    x='AGE',
    y='PAY_AMT1',
    c='LIMIT_BAL',
    cmap=plt.get_cmap("jet"),
    figsize=(10,10),
    colorbar=True
)


categorical_feats = [
    'PAY_0',
    'PAY_2',
    'PAY_3',
    'PAY_4',
    'PAY_5',
    'PAY_6',
    'MARRIAGE',
    'EDUCATION'
]

numeric_feats = [
    'LIMIT_BAL',
    'AGE',
    'BILL_AMT1',
    'BILL_AMT2',
    'BILL_AMT3',
    'BILL_AMT4',
    'BILL_AMT5',
    'BILL_AMT6',
    'PAY_AMT1',
    'PAY_AMT2',
    'PAY_AMT3',
    'PAY_AMT4',
    'PAY_AMT5',
    'PAY_AMT6',
]
    
binary_feats = [
    'SEX'
]


numeric_transformer = make_pipeline(StandardScaler())


categorical_transformer = make_pipeline(
    OneHotEncoder(handle_unknown="ignore", sparse=False),
)

binary_transformer = make_pipeline(
    OneHotEncoder(drop="if_binary", dtype=int),
)


preprocessor = make_column_transformer(
    (StandardScaler(), numeric_feats), 
    (OneHotEncoder(drop="if_binary", dtype=int), binary_feats),    
    (OneHotEncoder(handle_unknown="ignore", sparse=False), categorical_feats),
)

transformed = preprocessor.fit_transform(X_train, y_train)


def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):
    """
    Returns mean and std of cross validation

    Parameters
    ----------
    model :
        scikit-learn model
    X_train : numpy array or pandas DataFrame
        X in the training data
    y_train :
        y in the training data

    Returns
    ----------
        pandas Series with mean scores from cross_validation
    """

    scores = cross_validate(model, X_train, y_train, **kwargs)

    mean_scores = pd.DataFrame(scores).mean()
    std_scores = pd.DataFrame(scores).std()
    out_col = []

    for i in range(len(mean_scores)):
        out_col.append((f"%0.3f (+/- %0.3f)" % (mean_scores[i], std_scores[i])))

    return pd.Series(data=out_col, index=mean_scores.index)


scoring = [
    "f1",
    "recall",
    "accuracy"
]


results_dict = {}

dummy = DummyClassifier(strategy="prior")
pipe = make_pipeline(preprocessor, dummy)
results_dict["dummy"] = mean_std_cross_val_scores(
    pipe, X_train, y_train, cv=5, return_train_score=True, scoring = scoring
)
pd.DataFrame(results_dict)


# first attempt 

lr = LogisticRegression(max_iter=1000, class_weight='balanced')
scores = cross_validate(lr, X_train, y_train, return_train_score=True, scoring=scoring)
pd.DataFrame(scores).describe()


pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000, class_weight='balanced'))

param_grid = {
    "logisticregression__C": 2.0 ** np.arange(-4, 4)
}

search = GridSearchCV(
    pipe,
    param_grid,
    verbose=1,
    n_jobs=-1,
    return_train_score=True,
    scoring="f1",
)
search.fit(X_train, y_train);


grid_results_df = pd.DataFrame(search.cv_results_)[
    [
        "mean_test_score",
        "mean_train_score",
        "param_logisticregression__C",
        "rank_test_score",
    ]
]
grid_results_df = grid_results_df.sort_values(by="mean_test_score", ascending=False)
grid_results_df


grid_results_df.describe()



from catboost import CatBoostClassifier
from lightgbm.sklearn import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

pipe_lr = make_pipeline(
    preprocessor, LogisticRegression(max_iter=2000, random_state=123)
)
pipe_dt = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=123))
pipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=123))
pipe_xgb = make_pipeline(
    preprocessor, XGBClassifier(random_state=123, eval_metric="logloss", verbosity=0)
)
pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))
pipe_catboost = make_pipeline(
    preprocessor, CatBoostClassifier(verbose=0, random_state=123)
)
classifiers = {
    "logistic regression": pipe_lr,
    "decision tree": pipe_dt,
    "random forest": pipe_rf,
    "XGBoost": pipe_xgb,
    "LightGBM": pipe_lgbm,
    "CatBoost": pipe_catboost,
}


results = {}
import warnings

warnings.simplefilter(action="ignore", category=DeprecationWarning)
warnings.simplefilter(action="ignore", category=UserWarning)


for (name, model) in classifiers.items():
    results[name] = mean_std_cross_val_scores(
        model, X_train, y_train, return_train_score=True, scoring=scoring
    )


pd.DataFrame(results).T


lgbm_pipe = make_pipeline(preprocessor, LGBMClassifier(random_state=123))

param_grid = {
    "lgbmclassifier__max_depth": 10*np.arange(1, 50, 10),
    "lgbmclassifier__num_leaves": 2**np.arange(1, 50, 10),
}

lgbm_search = GridSearchCV(
    lgbm_pipe,
    param_grid,
    cv = 5,
    n_jobs=-1,
    return_train_score=True,
    scoring="f1",
)
lgbm_search.fit(X_train, y_train)


lgbm_search.best_params_


lgbm_search.best_score_


best_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000, class_weight='balanced', C = 0.0625))
best_pipe.fit(X_train, y_train)

coeffs = best_pipe.named_steps["logisticregression"].coef_.flatten()


preprocessor.named_transformers_


new_columns = (
    numeric_feats
    + binary_feats
    + list(
        preprocessor.named_transformers_["onehotencoder-2"].get_feature_names(
            categorical_feats
        )
    )
)


new_columns = (
    numeric_feats
    + binary_feats
    + list(
        preprocessor.named_transformers_["onehotencoder-2"].get_feature_names(
            categorical_feats
        )
    )
)
features = pd.DataFrame(coeffs, index=new_columns, columns=["Coefficient"])
features.sort_values(by = "Coefficient", ascending=False)


import mglearn
mglearn.tools.visualize_coefficients(coeffs, new_columns, n_top_features=10)


search.score(X_test, y_test)
